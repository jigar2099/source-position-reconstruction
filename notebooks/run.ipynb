{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b66f9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T15:22:27.445724Z",
     "start_time": "2023-05-07T15:22:23.608026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.unet_models import MODELS\n",
    "from src.tools import data_merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b99345",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T15:22:35.023341Z",
     "start_time": "2023-05-07T15:22:34.975431Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x = data_merger(dataset_type='train', data_type='src', noise_type='all', dirs = ['noise_7e-3'])\n",
    "train_y = data_merger(dataset_type='train', data_type='label', noise_type='all', dirs = ['noise_7e-3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2757f89c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T15:22:35.486441Z",
     "start_time": "2023-05-07T15:22:35.479458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2700, 32, 32) (2700, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55186e72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T15:22:37.212646Z",
     "start_time": "2023-05-07T15:22:36.943803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x161313a7df0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "samp = np.random.randint(4,train_x.shape[0],1)[0]\n",
    "plt.figure()\n",
    "plt.imshow(train_x[samp])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(train_y[samp])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76ddc1b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T15:27:28.421213Z",
     "start_time": "2023-05-07T15:27:28.415617Z"
    }
   },
   "outputs": [],
   "source": [
    "bins = 32\n",
    "channel = 1\n",
    "loss_func = 'mse'\n",
    "lr = '1e-4'\n",
    "opt = tf.keras.optimizers.Nadam(learning_rate=float(lr))\n",
    "metric = 'mae'\n",
    "reg = '0'\n",
    "filt_lst = [8,16,32,64]\n",
    "dns = 256\n",
    "drop = '0'\n",
    "batch_size = 256#\n",
    "ep=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6673058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T15:28:26.990735Z",
     "start_time": "2023-05-07T15:27:30.832532Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer output: (None, 32, 32, 1)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 32, 32, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 32, 32, 16)   160         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 32, 32, 16)   2320        ['conv2d_57[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_15 (MaxPooling2D  (None, 16, 16, 16)  0           ['conv2d_58[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 16, 16, 32)   4640        ['max_pooling2d_15[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 16, 16, 32)   0           ['conv2d_59[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 16, 16, 32)   9248        ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_16 (MaxPooling2D  (None, 8, 8, 32)    0           ['conv2d_60[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 8, 8, 64)     18496       ['max_pooling2d_16[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 8, 8, 64)     0           ['conv2d_61[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 8, 8, 64)     36928       ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_17 (MaxPooling2D  (None, 4, 4, 64)    0           ['conv2d_62[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 4, 4, 128)    73856       ['max_pooling2d_17[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4, 4, 128)    0           ['conv2d_63[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 4, 4, 128)    147584      ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_18 (MaxPooling2D  (None, 2, 2, 128)   0           ['conv2d_64[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 2, 2, 256)    295168      ['max_pooling2d_18[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 2, 2, 256)    0           ['conv2d_65[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 2, 2, 256)    590080      ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_12 (Conv2DTra  (None, 4, 4, 128)   131200      ['conv2d_66[0][0]']              \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 4, 4, 256)    0           ['conv2d_transpose_12[0][0]',    \n",
      "                                                                  'conv2d_64[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 4, 4, 128)    295040      ['concatenate_12[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_transpose_13 (Conv2DTra  (None, 8, 8, 64)    32832       ['conv2d_68[0][0]']              \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 8, 8, 128)    0           ['conv2d_transpose_13[0][0]',    \n",
      "                                                                  'conv2d_62[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 8, 8, 64)     73792       ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_transpose_14 (Conv2DTra  (None, 16, 16, 32)  8224        ['conv2d_70[0][0]']              \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 16, 16, 64)   0           ['conv2d_transpose_14[0][0]',    \n",
      "                                                                  'conv2d_60[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 16, 16, 32)   18464       ['concatenate_14[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_transpose_15 (Conv2DTra  (None, 32, 32, 16)  2064        ['conv2d_72[0][0]']              \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 32, 32, 32)   0           ['conv2d_transpose_15[0][0]',    \n",
      "                                                                  'conv2d_58[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 32, 32, 16)   4624        ['concatenate_15[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 32, 32, 1)    17          ['conv2d_74[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,744,737\n",
      "Trainable params: 1,744,737\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - ETA: 0s - loss: 0.0142 - mae: 0.0185\n",
      "Epoch 1: val_loss improved from inf to 0.01435, saving model to ../models/noise_7e-3\\Unet16_noise_7e-3_mse_lr_1e-4_500_DROPOUT0.h5\n",
      "8/8 [==============================] - 7s 293ms/step - loss: 0.0142 - mae: 0.0185 - val_loss: 0.0144 - val_mae: 0.0308\n",
      "Epoch 2/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0140 - mae: 0.0232\n",
      "Epoch 2: val_loss did not improve from 0.01435\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0140 - mae: 0.0232 - val_loss: 0.0144 - val_mae: 0.0395\n",
      "Epoch 3/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0138 - mae: 0.0260\n",
      "Epoch 3: val_loss did not improve from 0.01435\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0138 - mae: 0.0260 - val_loss: 0.0144 - val_mae: 0.0439\n",
      "Epoch 4/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0136 - mae: 0.0266\n",
      "Epoch 4: val_loss improved from 0.01435 to 0.01432, saving model to ../models/noise_7e-3\\Unet16_noise_7e-3_mse_lr_1e-4_500_DROPOUT0.h5\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0137 - mae: 0.0266 - val_loss: 0.0143 - val_mae: 0.0453\n",
      "Epoch 5/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0135 - mae: 0.0263\n",
      "Epoch 5: val_loss improved from 0.01432 to 0.01410, saving model to ../models/noise_7e-3\\Unet16_noise_7e-3_mse_lr_1e-4_500_DROPOUT0.h5\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0134 - mae: 0.0263 - val_loss: 0.0141 - val_mae: 0.0434\n",
      "Epoch 6/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0132 - mae: 0.0257\n",
      "Epoch 6: val_loss improved from 0.01410 to 0.01396, saving model to ../models/noise_7e-3\\Unet16_noise_7e-3_mse_lr_1e-4_500_DROPOUT0.h5\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0132 - mae: 0.0257 - val_loss: 0.0140 - val_mae: 0.0432\n",
      "Epoch 7/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0130 - mae: 0.0256\n",
      "Epoch 7: val_loss improved from 0.01396 to 0.01386, saving model to ../models/noise_7e-3\\Unet16_noise_7e-3_mse_lr_1e-4_500_DROPOUT0.h5\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0130 - mae: 0.0257 - val_loss: 0.0139 - val_mae: 0.0411\n",
      "Epoch 8/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0128 - mae: 0.0253\n",
      "Epoch 8: val_loss improved from 0.01386 to 0.01351, saving model to ../models/noise_7e-3\\Unet16_noise_7e-3_mse_lr_1e-4_500_DROPOUT0.h5\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0128 - mae: 0.0253 - val_loss: 0.0135 - val_mae: 0.0351\n",
      "Epoch 9/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0126 - mae: 0.0250\n",
      "Epoch 9: val_loss did not improve from 0.01351\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0126 - mae: 0.0250 - val_loss: 0.0137 - val_mae: 0.0423\n",
      "Epoch 10/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0124 - mae: 0.0252\n",
      "Epoch 10: val_loss did not improve from 0.01351\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0124 - mae: 0.0251 - val_loss: 0.0136 - val_mae: 0.0397\n",
      "Epoch 11/500\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0122 - mae: 0.0246\n",
      "Epoch 11: val_loss improved from 0.01351 to 0.01326, saving model to ../models/noise_7e-3\\Unet16_noise_7e-3_mse_lr_1e-4_500_DROPOUT0.h5\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0122 - mae: 0.0246 - val_loss: 0.0133 - val_mae: 0.0332\n",
      "Epoch 12/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0121 - mae: 0.0242\n",
      "Epoch 12: val_loss did not improve from 0.01326\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0121 - mae: 0.0242 - val_loss: 0.0135 - val_mae: 0.0406\n",
      "Epoch 13/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0120 - mae: 0.0241\n",
      "Epoch 13: val_loss improved from 0.01326 to 0.01310, saving model to ../models/noise_7e-3\\Unet16_noise_7e-3_mse_lr_1e-4_500_DROPOUT0.h5\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0119 - mae: 0.0241 - val_loss: 0.0131 - val_mae: 0.0308\n",
      "Epoch 14/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0118 - mae: 0.0233\n",
      "Epoch 14: val_loss did not improve from 0.01310\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0118 - mae: 0.0233 - val_loss: 0.0132 - val_mae: 0.0294\n",
      "Epoch 15/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0116 - mae: 0.0229\n",
      "Epoch 15: val_loss improved from 0.01310 to 0.01291, saving model to ../models/noise_7e-3\\Unet16_noise_7e-3_mse_lr_1e-4_500_DROPOUT0.h5\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0116 - mae: 0.0228 - val_loss: 0.0129 - val_mae: 0.0281\n",
      "Epoch 16/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0114 - mae: 0.0226\n",
      "Epoch 16: val_loss did not improve from 0.01291\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0114 - mae: 0.0226 - val_loss: 0.0132 - val_mae: 0.0372\n",
      "Epoch 17/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0113 - mae: 0.0227\n",
      "Epoch 17: val_loss did not improve from 0.01291\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0113 - mae: 0.0227 - val_loss: 0.0136 - val_mae: 0.0409\n",
      "Epoch 18/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0111 - mae: 0.0221\n",
      "Epoch 18: val_loss improved from 0.01291 to 0.01272, saving model to ../models/noise_7e-3\\Unet16_noise_7e-3_mse_lr_1e-4_500_DROPOUT0.h5\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0111 - mae: 0.0222 - val_loss: 0.0127 - val_mae: 0.0287\n",
      "Epoch 19/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0109 - mae: 0.0215\n",
      "Epoch 19: val_loss did not improve from 0.01272\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0109 - mae: 0.0215 - val_loss: 0.0134 - val_mae: 0.0387\n",
      "Epoch 20/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0108 - mae: 0.0216\n",
      "Epoch 20: val_loss improved from 0.01272 to 0.01248, saving model to ../models/noise_7e-3\\Unet16_noise_7e-3_mse_lr_1e-4_500_DROPOUT0.h5\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0108 - mae: 0.0216 - val_loss: 0.0125 - val_mae: 0.0238\n",
      "Epoch 21/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0107 - mae: 0.0208\n",
      "Epoch 21: val_loss did not improve from 0.01248\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0107 - mae: 0.0208 - val_loss: 0.0130 - val_mae: 0.0326\n",
      "Epoch 22/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0105 - mae: 0.0208\n",
      "Epoch 22: val_loss did not improve from 0.01248\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0105 - mae: 0.0208 - val_loss: 0.0129 - val_mae: 0.0319\n",
      "Epoch 23/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0104 - mae: 0.0205\n",
      "Epoch 23: val_loss did not improve from 0.01248\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0104 - mae: 0.0205 - val_loss: 0.0127 - val_mae: 0.0281\n",
      "Epoch 24/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0103 - mae: 0.0202\n",
      "Epoch 24: val_loss did not improve from 0.01248\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0103 - mae: 0.0202 - val_loss: 0.0129 - val_mae: 0.0308\n",
      "Epoch 25/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0102 - mae: 0.0200\n",
      "Epoch 25: val_loss did not improve from 0.01248\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0102 - mae: 0.0200 - val_loss: 0.0126 - val_mae: 0.0263\n",
      "Epoch 26/500\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0101 - mae: 0.0196\n",
      "Epoch 26: val_loss improved from 0.01248 to 0.01227, saving model to ../models/noise_7e-3\\Unet16_noise_7e-3_mse_lr_1e-4_500_DROPOUT0.h5\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0101 - mae: 0.0196 - val_loss: 0.0123 - val_mae: 0.0202\n",
      "Epoch 27/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0100 - mae: 0.0191\n",
      "Epoch 27: val_loss did not improve from 0.01227\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0099 - mae: 0.0191 - val_loss: 0.0123 - val_mae: 0.0208\n",
      "Epoch 28/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0098 - mae: 0.0188\n",
      "Epoch 28: val_loss did not improve from 0.01227\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0098 - mae: 0.0188 - val_loss: 0.0127 - val_mae: 0.0284\n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - ETA: 0s - loss: 0.0097 - mae: 0.0191\n",
      "Epoch 29: val_loss did not improve from 0.01227\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0097 - mae: 0.0191 - val_loss: 0.0127 - val_mae: 0.0266\n",
      "Epoch 30/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0096 - mae: 0.0188\n",
      "Epoch 30: val_loss did not improve from 0.01227\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0096 - mae: 0.0188 - val_loss: 0.0126 - val_mae: 0.0257\n",
      "Epoch 31/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0095 - mae: 0.0185\n",
      "Epoch 31: val_loss did not improve from 0.01227\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0095 - mae: 0.0185 - val_loss: 0.0128 - val_mae: 0.0282\n",
      "Epoch 32/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0094 - mae: 0.0184\n",
      "Epoch 32: val_loss improved from 0.01227 to 0.01213, saving model to ../models/noise_7e-3\\Unet16_noise_7e-3_mse_lr_1e-4_500_DROPOUT0.h5\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0094 - mae: 0.0183 - val_loss: 0.0121 - val_mae: 0.0202\n",
      "Epoch 33/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0093 - mae: 0.0179\n",
      "Epoch 33: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0093 - mae: 0.0179 - val_loss: 0.0125 - val_mae: 0.0242\n",
      "Epoch 34/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0092 - mae: 0.0177\n",
      "Epoch 34: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0092 - mae: 0.0177 - val_loss: 0.0130 - val_mae: 0.0293\n",
      "Epoch 35/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0092 - mae: 0.0180\n",
      "Epoch 35: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0091 - mae: 0.0180 - val_loss: 0.0126 - val_mae: 0.0258\n",
      "Epoch 36/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0090 - mae: 0.0175\n",
      "Epoch 36: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0090 - mae: 0.0175 - val_loss: 0.0128 - val_mae: 0.0261\n",
      "Epoch 37/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0089 - mae: 0.0172\n",
      "Epoch 37: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0090 - mae: 0.0172 - val_loss: 0.0123 - val_mae: 0.0221\n",
      "Epoch 38/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0089 - mae: 0.0168\n",
      "Epoch 38: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0089 - mae: 0.0168 - val_loss: 0.0129 - val_mae: 0.0275\n",
      "Epoch 39/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0087 - mae: 0.0170\n",
      "Epoch 39: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0087 - mae: 0.0170 - val_loss: 0.0135 - val_mae: 0.0304\n",
      "Epoch 40/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0086 - mae: 0.0170\n",
      "Epoch 40: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0087 - mae: 0.0170 - val_loss: 0.0140 - val_mae: 0.0333\n",
      "Epoch 41/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0086 - mae: 0.0170\n",
      "Epoch 41: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0086 - mae: 0.0170 - val_loss: 0.0162 - val_mae: 0.0422\n",
      "Epoch 42/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0087 - mae: 0.0169\n",
      "Epoch 42: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0087 - mae: 0.0169 - val_loss: 0.0140 - val_mae: 0.0327\n",
      "Epoch 43/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0084 - mae: 0.0165\n",
      "Epoch 43: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0084 - mae: 0.0165 - val_loss: 0.0125 - val_mae: 0.0233\n",
      "Epoch 44/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0083 - mae: 0.0160\n",
      "Epoch 44: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0083 - mae: 0.0160 - val_loss: 0.0123 - val_mae: 0.0208\n",
      "Epoch 45/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0083 - mae: 0.0158\n",
      "Epoch 45: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0083 - mae: 0.0158 - val_loss: 0.0131 - val_mae: 0.0264\n",
      "Epoch 46/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0082 - mae: 0.0160\n",
      "Epoch 46: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0082 - mae: 0.0160 - val_loss: 0.0138 - val_mae: 0.0293\n",
      "Epoch 47/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0082 - mae: 0.0159\n",
      "Epoch 47: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0081 - mae: 0.0159 - val_loss: 0.0132 - val_mae: 0.0269\n",
      "Epoch 48/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0080 - mae: 0.0155\n",
      "Epoch 48: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0080 - mae: 0.0155 - val_loss: 0.0133 - val_mae: 0.0265\n",
      "Epoch 49/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0080 - mae: 0.0153\n",
      "Epoch 49: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0079 - mae: 0.0153 - val_loss: 0.0131 - val_mae: 0.0251\n",
      "Epoch 50/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0079 - mae: 0.0152\n",
      "Epoch 50: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0079 - mae: 0.0152 - val_loss: 0.0132 - val_mae: 0.0254\n",
      "Epoch 51/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0079 - mae: 0.0151\n",
      "Epoch 51: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0079 - mae: 0.0151 - val_loss: 0.0125 - val_mae: 0.0215\n",
      "Epoch 52/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0078 - mae: 0.0147\n",
      "Epoch 52: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0078 - mae: 0.0147 - val_loss: 0.0125 - val_mae: 0.0216\n",
      "Epoch 53/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0077 - mae: 0.0146\n",
      "Epoch 53: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0077 - mae: 0.0146 - val_loss: 0.0125 - val_mae: 0.0215\n",
      "Epoch 54/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0076 - mae: 0.0146\n",
      "Epoch 54: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0077 - mae: 0.0146 - val_loss: 0.0143 - val_mae: 0.0297\n",
      "Epoch 55/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0076 - mae: 0.0148\n",
      "Epoch 55: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0076 - mae: 0.0148 - val_loss: 0.0137 - val_mae: 0.0271\n",
      "Epoch 56/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0074 - mae: 0.0145\n",
      "Epoch 56: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0075 - mae: 0.0145 - val_loss: 0.0140 - val_mae: 0.0273\n",
      "Epoch 57/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0075 - mae: 0.0144\n",
      "Epoch 57: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0075 - mae: 0.0144 - val_loss: 0.0124 - val_mae: 0.0195\n",
      "Epoch 58/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0075 - mae: 0.0141\n",
      "Epoch 58: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0075 - mae: 0.0141 - val_loss: 0.0150 - val_mae: 0.0307\n",
      "Epoch 59/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0073 - mae: 0.0142\n",
      "Epoch 59: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0073 - mae: 0.0142 - val_loss: 0.0145 - val_mae: 0.0285\n",
      "Epoch 60/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0073 - mae: 0.0142\n",
      "Epoch 60: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0073 - mae: 0.0142 - val_loss: 0.0140 - val_mae: 0.0267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0072 - mae: 0.0138\n",
      "Epoch 61: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0072 - mae: 0.0138 - val_loss: 0.0135 - val_mae: 0.0240\n",
      "Epoch 62/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0072 - mae: 0.0136\n",
      "Epoch 62: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0072 - mae: 0.0136 - val_loss: 0.0133 - val_mae: 0.0232\n",
      "Epoch 63/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0071 - mae: 0.0135\n",
      "Epoch 63: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0071 - mae: 0.0135 - val_loss: 0.0142 - val_mae: 0.0269\n",
      "Epoch 64/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0070 - mae: 0.0135\n",
      "Epoch 64: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0070 - mae: 0.0135 - val_loss: 0.0133 - val_mae: 0.0225\n",
      "Epoch 65/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0070 - mae: 0.0132\n",
      "Epoch 65: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0070 - mae: 0.0132 - val_loss: 0.0158 - val_mae: 0.0306\n",
      "Epoch 66/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0069 - mae: 0.0133\n",
      "Epoch 66: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0069 - mae: 0.0133 - val_loss: 0.0153 - val_mae: 0.0287\n",
      "Epoch 67/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0069 - mae: 0.0131\n",
      "Epoch 67: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0069 - mae: 0.0131 - val_loss: 0.0142 - val_mae: 0.0257\n",
      "Epoch 68/500\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0069 - mae: 0.0131\n",
      "Epoch 68: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0069 - mae: 0.0131 - val_loss: 0.0150 - val_mae: 0.0278\n",
      "Epoch 69/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0067 - mae: 0.0129\n",
      "Epoch 69: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0067 - mae: 0.0129 - val_loss: 0.0149 - val_mae: 0.0270\n",
      "Epoch 70/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0067 - mae: 0.0128\n",
      "Epoch 70: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0067 - mae: 0.0128 - val_loss: 0.0141 - val_mae: 0.0244\n",
      "Epoch 71/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0067 - mae: 0.0127\n",
      "Epoch 71: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0067 - mae: 0.0127 - val_loss: 0.0152 - val_mae: 0.0277\n",
      "Epoch 72/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0066 - mae: 0.0126\n",
      "Epoch 72: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0066 - mae: 0.0127 - val_loss: 0.0163 - val_mae: 0.0309\n",
      "Epoch 73/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0066 - mae: 0.0127\n",
      "Epoch 73: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0066 - mae: 0.0127 - val_loss: 0.0183 - val_mae: 0.0356\n",
      "Epoch 74/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0066 - mae: 0.0128\n",
      "Epoch 74: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0066 - mae: 0.0128 - val_loss: 0.0194 - val_mae: 0.0385\n",
      "Epoch 75/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0066 - mae: 0.0128\n",
      "Epoch 75: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0066 - mae: 0.0128 - val_loss: 0.0166 - val_mae: 0.0312\n",
      "Epoch 76/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0064 - mae: 0.0124\n",
      "Epoch 76: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0064 - mae: 0.0124 - val_loss: 0.0183 - val_mae: 0.0360\n",
      "Epoch 77/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0064 - mae: 0.0126\n",
      "Epoch 77: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0064 - mae: 0.0126 - val_loss: 0.0187 - val_mae: 0.0357\n",
      "Epoch 78/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0064 - mae: 0.0124\n",
      "Epoch 78: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0064 - mae: 0.0124 - val_loss: 0.0154 - val_mae: 0.0272\n",
      "Epoch 79/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0064 - mae: 0.0122\n",
      "Epoch 79: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0064 - mae: 0.0122 - val_loss: 0.0150 - val_mae: 0.0262\n",
      "Epoch 80/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0063 - mae: 0.0121\n",
      "Epoch 80: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0063 - mae: 0.0121 - val_loss: 0.0179 - val_mae: 0.0336\n",
      "Epoch 81/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0063 - mae: 0.0121\n",
      "Epoch 81: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0063 - mae: 0.0121 - val_loss: 0.0168 - val_mae: 0.0308\n",
      "Epoch 82/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0062 - mae: 0.0121\n",
      "Epoch 82: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0062 - mae: 0.0121 - val_loss: 0.0181 - val_mae: 0.0337\n",
      "Epoch 83/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0062 - mae: 0.0120\n",
      "Epoch 83: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0062 - mae: 0.0120 - val_loss: 0.0237 - val_mae: 0.0451\n",
      "Epoch 84/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0063 - mae: 0.0123\n",
      "Epoch 84: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0063 - mae: 0.0123 - val_loss: 0.0196 - val_mae: 0.0371\n",
      "Epoch 85/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0061 - mae: 0.0119\n",
      "Epoch 85: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0061 - mae: 0.0119 - val_loss: 0.0180 - val_mae: 0.0333\n",
      "Epoch 86/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0060 - mae: 0.0117\n",
      "Epoch 86: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0060 - mae: 0.0117 - val_loss: 0.0214 - val_mae: 0.0407\n",
      "Epoch 87/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0061 - mae: 0.0121\n",
      "Epoch 87: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0061 - mae: 0.0120 - val_loss: 0.0183 - val_mae: 0.0337\n",
      "Epoch 88/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0059 - mae: 0.0117\n",
      "Epoch 88: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0060 - mae: 0.0117 - val_loss: 0.0227 - val_mae: 0.0429\n",
      "Epoch 89/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0061 - mae: 0.0118\n",
      "Epoch 89: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0061 - mae: 0.0119 - val_loss: 0.0192 - val_mae: 0.0353\n",
      "Epoch 90/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0059 - mae: 0.0116\n",
      "Epoch 90: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0059 - mae: 0.0116 - val_loss: 0.0181 - val_mae: 0.0328\n",
      "Epoch 91/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0059 - mae: 0.0113\n",
      "Epoch 91: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0059 - mae: 0.0113 - val_loss: 0.0194 - val_mae: 0.0350\n",
      "Epoch 92/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0058 - mae: 0.0113\n",
      "Epoch 92: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0059 - mae: 0.0113 - val_loss: 0.0184 - val_mae: 0.0328\n",
      "Epoch 93/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0058 - mae: 0.0113\n",
      "Epoch 93: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0058 - mae: 0.0113 - val_loss: 0.0214 - val_mae: 0.0389\n",
      "Epoch 94/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0058 - mae: 0.0113\n",
      "Epoch 94: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0057 - mae: 0.0113 - val_loss: 0.0189 - val_mae: 0.0336\n",
      "Epoch 95/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0057 - mae: 0.0111\n",
      "Epoch 95: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0057 - mae: 0.0111 - val_loss: 0.0214 - val_mae: 0.0386\n",
      "Epoch 96/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0057 - mae: 0.0113\n",
      "Epoch 96: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0057 - mae: 0.0113 - val_loss: 0.0254 - val_mae: 0.0464\n",
      "Epoch 97/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0058 - mae: 0.0114\n",
      "Epoch 97: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0058 - mae: 0.0114 - val_loss: 0.0219 - val_mae: 0.0398\n",
      "Epoch 98/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0057 - mae: 0.0112\n",
      "Epoch 98: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0057 - mae: 0.0112 - val_loss: 0.0262 - val_mae: 0.0483\n",
      "Epoch 99/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0057 - mae: 0.0113\n",
      "Epoch 99: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0057 - mae: 0.0113 - val_loss: 0.0219 - val_mae: 0.0390\n",
      "Epoch 100/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0056 - mae: 0.0109\n",
      "Epoch 100: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0055 - mae: 0.0109 - val_loss: 0.0208 - val_mae: 0.0368\n",
      "Epoch 101/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0056 - mae: 0.0109\n",
      "Epoch 101: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0056 - mae: 0.0108 - val_loss: 0.0229 - val_mae: 0.0404\n",
      "Epoch 102/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0055 - mae: 0.0109\n",
      "Epoch 102: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0055 - mae: 0.0109 - val_loss: 0.0240 - val_mae: 0.0429\n",
      "Epoch 103/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0055 - mae: 0.0109\n",
      "Epoch 103: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0055 - mae: 0.0109 - val_loss: 0.0243 - val_mae: 0.0425\n",
      "Epoch 104/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0055 - mae: 0.0108\n",
      "Epoch 104: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0054 - mae: 0.0107 - val_loss: 0.0219 - val_mae: 0.0380\n",
      "Epoch 105/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0054 - mae: 0.0107\n",
      "Epoch 105: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0054 - mae: 0.0107 - val_loss: 0.0224 - val_mae: 0.0385\n",
      "Epoch 106/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0054 - mae: 0.0105\n",
      "Epoch 106: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0054 - mae: 0.0105 - val_loss: 0.0244 - val_mae: 0.0416\n",
      "Epoch 107/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0053 - mae: 0.0105\n",
      "Epoch 107: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0053 - mae: 0.0105 - val_loss: 0.0234 - val_mae: 0.0397\n",
      "Epoch 108/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0054 - mae: 0.0106\n",
      "Epoch 108: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0054 - mae: 0.0105 - val_loss: 0.0222 - val_mae: 0.0382\n",
      "Epoch 109/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0053 - mae: 0.0104\n",
      "Epoch 109: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0053 - mae: 0.0103 - val_loss: 0.0226 - val_mae: 0.0380\n",
      "Epoch 110/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0053 - mae: 0.0103\n",
      "Epoch 110: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0053 - mae: 0.0103 - val_loss: 0.0205 - val_mae: 0.0346\n",
      "Epoch 111/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0052 - mae: 0.0101\n",
      "Epoch 111: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0052 - mae: 0.0101 - val_loss: 0.0238 - val_mae: 0.0400\n",
      "Epoch 112/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0052 - mae: 0.0103\n",
      "Epoch 112: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0052 - mae: 0.0103 - val_loss: 0.0295 - val_mae: 0.0504\n",
      "Epoch 113/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0054 - mae: 0.0107\n",
      "Epoch 113: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0054 - mae: 0.0107 - val_loss: 0.0253 - val_mae: 0.0429\n",
      "Epoch 114/500\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0052 - mae: 0.0102\n",
      "Epoch 114: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0052 - mae: 0.0102 - val_loss: 0.0262 - val_mae: 0.0443\n",
      "Epoch 115/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0051 - mae: 0.0101\n",
      "Epoch 115: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0051 - mae: 0.0101 - val_loss: 0.0299 - val_mae: 0.0499\n",
      "Epoch 116/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0052 - mae: 0.0102\n",
      "Epoch 116: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0051 - mae: 0.0102 - val_loss: 0.0233 - val_mae: 0.0388\n",
      "Epoch 117/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0050 - mae: 0.0098\n",
      "Epoch 117: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0050 - mae: 0.0098 - val_loss: 0.0252 - val_mae: 0.0419\n",
      "Epoch 118/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0050 - mae: 0.0099\n",
      "Epoch 118: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0050 - mae: 0.0099 - val_loss: 0.0261 - val_mae: 0.0423\n",
      "Epoch 119/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0050 - mae: 0.0099\n",
      "Epoch 119: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0050 - mae: 0.0099 - val_loss: 0.0219 - val_mae: 0.0360\n",
      "Epoch 120/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0049 - mae: 0.0096\n",
      "Epoch 120: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0049 - mae: 0.0096 - val_loss: 0.0214 - val_mae: 0.0343\n",
      "Epoch 121/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0049 - mae: 0.0095\n",
      "Epoch 121: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0050 - mae: 0.0096 - val_loss: 0.0233 - val_mae: 0.0383\n",
      "Epoch 122/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0049 - mae: 0.0095\n",
      "Epoch 122: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0049 - mae: 0.0095 - val_loss: 0.0231 - val_mae: 0.0374\n",
      "Epoch 123/500\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0048 - mae: 0.0095\n",
      "Epoch 123: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0048 - mae: 0.0095 - val_loss: 0.0279 - val_mae: 0.0450\n",
      "Epoch 124/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0049 - mae: 0.0096\n",
      "Epoch 124: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0049 - mae: 0.0096 - val_loss: 0.0288 - val_mae: 0.0464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0048 - mae: 0.0096\n",
      "Epoch 125: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0048 - mae: 0.0096 - val_loss: 0.0331 - val_mae: 0.0533\n",
      "Epoch 126/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0049 - mae: 0.0097\n",
      "Epoch 126: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 0.0048 - mae: 0.0097 - val_loss: 0.0259 - val_mae: 0.0413\n",
      "Epoch 127/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0047 - mae: 0.0092\n",
      "Epoch 127: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0047 - mae: 0.0092 - val_loss: 0.0318 - val_mae: 0.0493\n",
      "Epoch 128/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0047 - mae: 0.0093\n",
      "Epoch 128: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0047 - mae: 0.0094 - val_loss: 0.0303 - val_mae: 0.0479\n",
      "Epoch 129/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0047 - mae: 0.0093\n",
      "Epoch 129: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0047 - mae: 0.0093 - val_loss: 0.0268 - val_mae: 0.0425\n",
      "Epoch 130/500\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.0047 - mae: 0.0092\n",
      "Epoch 130: val_loss did not improve from 0.01213\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0047 - mae: 0.0092 - val_loss: 0.0303 - val_mae: 0.0477\n",
      "Epoch 131/500\n",
      "3/8 [==========>...................] - ETA: 0s - loss: 0.0048 - mae: 0.0097"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m mc \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_func\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_DROPOUT\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     31\u001b[0m                                        monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 34\u001b[0m history\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.33\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_model = MODELS()\n",
    "model_type=\"Unet16\"\n",
    "\n",
    "if model_type==\"attention_resunet\":\n",
    "    FILTER_NUM = 16\n",
    "    FILTER_SIZE = 3\n",
    "    NUM_CLASSES = 1\n",
    "    dropout_rate = 0.2\n",
    "    batch_norm = False\n",
    "    model = get_model.Attention_ResUNet(bins,channel,loss_func,opt,metric,FILTER_NUM=FILTER_NUM,batch_norm = False, dropout_rate = float(drop))\n",
    "elif model_type==\"unetpp\":\n",
    "    nb_filter = [1, 32, 64, 128, 256]\n",
    "    deep_supervision = False\n",
    "    model = get_model.Nest_Net(bins,channel,loss_func,opt,metric,nb_filter=nb_filter,deep_supervision=deep_supervision)\n",
    "elif model_type==\"Unet16\":\n",
    "    filt_num = 16\n",
    "    model = get_model.UNET(bins, channel, loss_func, opt, metric, float(reg), filt_num)\n",
    "elif model_type==\"vgg16\":\n",
    "    model = get_model.VGG16(filt_lst,dns,bins,channel,loss_func,opt,metric)\n",
    "elif model_type==\"resUnet\":\n",
    "    model = get_model.resUnet(bins, channel, loss_func, opt, metric, float(reg))\n",
    "elif model_type==\"unetPP48163264\":\n",
    "    nb_filter = [4,8,16,32,64]\n",
    "    filters = [4,8,16,32,64]\n",
    "    model = get_model.unetPP(bins,channel,loss_func,opt,metric,float(reg), nb_filter, filters)\n",
    "\n",
    "folder_name = 'noise_7e-3'\n",
    "chnl = 'ch0'\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(f'../models/{folder_name}/{model_type}_{folder_name}_{loss_func}_lr_{lr}_{ep}_DROPOUT{reg}.h5',\n",
    "                                       monitor='val_loss', model='min', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "history=model.fit(train_x[:],train_y[:],\n",
    "                batch_size=batch_size,epochs=ep,\n",
    "                validation_split=0.33,shuffle=True, callbacks=[callback,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe66ac8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-06T07:33:02.734293Z",
     "start_time": "2023-05-06T07:33:02.724796Z"
    }
   },
   "outputs": [],
   "source": [
    "train_X = train_x.reshape(train_x.shape[0],32,32,1)#/np.max(train_x)\n",
    "train_Y = train_y.reshape(train_y.shape[0],32,32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c2cc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T21:40:19.000561Z",
     "start_time": "2023-05-03T21:40:18.655743Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b594b5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-06T07:32:03.594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2500\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0038 - mae: 0.0068\n",
      "Epoch 1: val_loss did not improve from 0.01549\n",
      "32/32 [==============================] - 6s 81ms/step - loss: 0.0038 - mae: 0.0068 - val_loss: 0.0159 - val_mae: 0.0218\n",
      "Epoch 2/2500\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.0037 - mae: 0.0065\n",
      "Epoch 2: val_loss did not improve from 0.01549\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 0.0037 - mae: 0.0065 - val_loss: 0.0161 - val_mae: 0.0213\n",
      "Epoch 3/2500\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.0035 - mae: 0.0063\n",
      "Epoch 3: val_loss did not improve from 0.01549\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.0035 - mae: 0.0063 - val_loss: 0.0161 - val_mae: 0.0220\n",
      "Epoch 4/2500\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0034 - mae: 0.0061\n",
      "Epoch 4: val_loss did not improve from 0.01549\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.0034 - mae: 0.0061 - val_loss: 0.0169 - val_mae: 0.0245\n",
      "Epoch 5/2500\n",
      "29/32 [==========================>...] - ETA: 0s - loss: 0.0034 - mae: 0.0060"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 2500\n",
    "history = model.fit(train_X, train_Y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          #validation_split=.33,\n",
    "          validation_split = 0.33,          \n",
    "          verbose=1,\n",
    "          callbacks=[callback, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98cccc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
